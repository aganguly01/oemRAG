High-Level Data Flow Overview for RAG-based Knowledge Assistant Demo
Objective
To showcase how modern AI-driven search and question-answering capabilities can be built by combining document embeddings, vector search, and language models, helping users quickly find insights from large, diverse data sources.

Data Sources
We collect and consolidate example content from various production-relevant domains, including:

Monitoring logs (e.g., Splunk, New Relic)

Database operational info (MongoDB, DynamoDB)

Internal documentation (Confluence pages on processes, escalation, security)

Service health and incident reports (service availability, response time issues)

Security-related data (client IP tracking, payload encryption)

These represent the kinds of real-world questions users may ask during incident investigation or troubleshooting.

Data Processing and Embeddings
Document Vectorization
Each document (a piece of text) is converted into a numerical vector (embedding) using a text embedding model. This vector captures the semantic meaning of the document in a way that similar texts have similar vectors.

Vector Store (Chroma DB)
These vectors, along with the original documents, are stored in a vector database that allows efficient similarity search. When a query is input, it is also embedded and then compared against stored document vectors to find the closest matches.

Query Handling: LLM vs Non-LLM Modes
Non-LLM Mode:
The system performs vector similarity search to retrieve the top relevant documents (contexts) matching the user query. The answer is generated by simply returning or summarizing these retrieved contexts.

LLM Mode:
The top relevant documents are combined and fed as context into a Language Model (like flan-t5-small), which generates a natural, coherent answer by reasoning over the combined information. This provides a more conversational and human-like response.

User Interaction Flow
User enters a question in the Streamlit web UI.

The query is embedded and used to search for the top 3 semantically similar documents in the vector database.

If LLM mode is ON, the retrieved contexts plus the question are passed to the LLM to generate a final answer.

If LLM mode is OFF, the system returns the raw closest context documents as the answer.

The UI shows the answer and the relevant source documents for transparency and further exploration.

Benefits & Value
Faster Issue Resolution: Users can ask natural language questions and get instant, relevant information across many data sources.

Contextual & Explainable Answers: The system shows both answers and their source contexts to increase trust.

Flexible Deployment: The demo supports running with or without LLMs, accommodating different governance and resource constraints.

Scalable Architecture: Adding new data sources and documents is straightforward, enabling continuous improvement.

Next Steps
Integration with live production logs and documentation.

Enhancing the LLM with fine-tuning or advanced retrieval techniques (e.g., refine, map-reduce).

Incorporating feedback and analytics to improve answer quality over time.

#One pager

AI-Powered Knowledge Assistant: High-Level Overview
Purpose
Enable fast, accurate, and context-aware answers from diverse operational data sources using modern AI techniques — combining semantic search with language models.

Data Sources
Monitoring Logs: Splunk, New Relic (service availability, response times)

Databases: MongoDB, DynamoDB (performance spikes, connection issues)

Documentation: Confluence pages (escalation process, security protocols)

Security Events: Client IP tracking, payload encryption

Incident Reports: Real-world production incidents

Core Components
Component	Description
Vector Embeddings	Converts text documents into numeric vectors capturing meaning
Vector Database (ChromaDB)	Stores embeddings enabling fast similarity search
Query Handling	User query → vectorized → search top relevant docs
Language Model (LLM)	Optional: Uses retrieved docs + query to generate human-like answers

Modes of Operation
Non-LLM Mode: Returns closest matching documents as answers directly.

LLM Mode: Feeds retrieved documents and query to a local transformer model (e.g., Flan-T5) for refined, conversational responses.

User Flow
User enters question in a simple web interface (Streamlit).

System searches vector DB for top 3 relevant documents.

If LLM enabled, generates answer using language model + contexts; else returns raw context.

Displays answer + source documents for transparency.

Benefits
Speeds up troubleshooting and knowledge discovery.

Combines multiple data sources seamlessly.

Transparent: users see source contexts.

Flexible: runs with or without LLM, accommodating security policies.

Next Steps
Expand data ingestion for live environments.

Improve answer quality with advanced retrieval and model fine-tuning.

Add feedback loops for continuous learning.

This demo illustrates how Retrieval-Augmented Generation (RAG) can transform enterprise support by bridging large document collections with AI-powered Q&A.
